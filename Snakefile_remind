# SPDX-FileCopyrightText: : 2023 Adrian Odenweller <adrian.odenweller@pik-potsdam.de>, Johannes Hampp <johannes.hampp@pik-potsdam.de>
#
# SPDX-License-Identifier: MIT
from os.path import normpath, exists
from shutil import copyfile, move, rmtree
import numpy as np
import pandas as pd
from snakemake.remote.HTTP import RemoteProvider as HTTPRemoteProvider

HTTP = HTTPRemoteProvider()

from snakemake.utils import min_version

min_version("7.7")


localrules:
    purge,
    dag,
    download_and_prepare,
    determine_co2_price_scenarios,


wildcard_constraints:
    simpl="[a-zA-Z0-9]*",
    clusters="[0-9]+m?|all",
    ll="(v|c)([0-9\.]+|opt)",
    opts="[-+a-zA-Z0-9\.]*",
    year="2[01][0-9][05]",
    iteration="[0-9]+",


configfile: "config.remind.yaml"


COSTS = f"data/costs_{config['costs']['year']}.csv"
ATLITE_NPROCESSES = config["atlite"].get("nprocesses", 4)

run = config.get("run", {})
RDIR = run["name"] + "/" if run.get("name") else ""
CDIR = RDIR if not run.get("shared_cutouts") else ""

LOGS = "logs/" + RDIR if not run.get("shared_resources") else "resources/"
BENCHMARKS = "benchmarks/" + RDIR if not run.get("shared_resources") else "resources/"
RESOURCES = "resources/" + RDIR if not run.get("shared_resources") else "resources/"
RESULTS = "results/" + RDIR if not run.get("shared_resources") else "resources/"


include: "rules/common.smk"
include: "rules/collect.smk"
include: "rules/retrieve.smk"
include: "rules/build_electricity.smk"
include: "rules/solve_electricity.smk"
include: "rules/postprocess.smk"


rule purge:
    message:
        "Purging generated resources, results and docs. Downloads are kept."
    run:
        rmtree("resources/", ignore_errors=True)
        rmtree("results/", ignore_errors=True)
        rmtree("doc/_build", ignore_errors=True)


rule dag:
    message:
        "Creating DAG of workflow."
    output:
        dot=RESOURCES + "dag.dot",
        pdf=RESOURCES + "dag.pdf",
        png=RESOURCES + "dag.png",
    conda:
        "envs/environment.yaml"
    shell:
        """
        snakemake --rulegraph all | sed -n "/digraph/,\$p" > {output.dot}
        dot -Tpdf -o {output.pdf} {output.dot}
        dot -Tpng -o {output.png} {output.dot}
        """


# # Import PyPSA-EUR standard workflow as module
# # Overwrite or inherit from rules below to adjust for REMIND coupling
# module pypsa_eur:
#     snakefile:
#         # The unmodified PyPSA-EUR Snakefile
#         "Snakefile"
#     config:
#         # "config.remind.yaml" is a modified version of the PyPSA-EUR config.yaml
#         # and can directly be passed to the module
#         config


# # Import all rules from pypsa_eur module with prefix
# use rule * from pypsa_eur as pypsa_eur_*


# # Variables from pypsa_eur module, alias here for better readibility
# BENCHMARKS = pypsa_eur.BENCHMARKS
# LOGS = pypsa_eur.LOGS
# RESOURCES = pypsa_eur.RESOURCES
# RESULTS = pypsa_eur.RESULTS


# Rule which downloads and prepares all files which will not change
# during REMIND <-> PyPSA-EUR coupled iterations and repeating runs.
# * Use for downloading on e.g. login node with internet connection
# * will create some load ~30m - 1h for creating renewable profiles
# run with:
# > snakemake -call -s Snakefile_remind download_and_prepare
rule download_and_prepare:
    message:
        "Downloading and preparing all single-execution rules. "
        "Running this rule requires an internet connection for downloading "
        "datafiles for the model."
    input:
        expand(
            rules.retrieve_cost_data.output[0],
            year=[2020, 2025, 2030, 2035, 2040, 2045, 2050],
        ),
        **{
            k: v
            for k, v in rules.add_electricity.input.items()
            if "{scenario}" not in v
        },


# # This part contains all rules, which get data from REMIND between iterations.
# # PyPSA-EUR networks are solved between iterations via snakemake calls:
# # > snakemake -call -s Snakefile_remind --config iteration=<iteration> -- remind_solve_all_networks
# #
# # Most of the rules below are rules inherited from PyPSA-EUR, where input files change
# # during the iterations between REMIND and PyPSA-EUR; therefore the input/output files of the rules
# # are modified (but not the model setup itself, i.e. Python scripts).
# # Some additional rules are created for helping to setup the model or to export results for use later by REMIND.
# #
# # Naming:
# # * Original rules from PyPSA-EUR are prefixed as: pypsa_eur_<rulename>
# # * New rules and modified rules (via inheritance "use rule ... as ... with") are prefixed as: remind_<rulename>
# #
# # Some of the new rules have additional wildcards:
# # * {year}: Indicating the REMIND year for which the model is built
# # * {iteration}: Indicating the REMIND iteration for which the model is built


# Build cost data compatible with the original PyPSA-EUR cost data structure from technology-data
# as part of the coupling with REMIND.
# Since the cost data is used in multiple rules, the cost data is provided as a dedicated file
# and not only overwritten in the network as part of the "remind_overwrite_elec_network" rule below.
rule import_REMIND_costs:
    input:
        # no TD data for < 2020 and > 2050
        original_costs=lambda w: f"data/costs_{np.clip(int(w['year']), a_min=2020, a_max=2050)}.csv",
        region_mapping="config/regionmapping_21_EU11.csv",
        remind_data="resources/{scenario}/i{iteration}/REMIND2PyPSAEUR.gdx",
    output:
        costs="resources/{scenario}/i{iteration}/y{year}/costs.csv",
    log:
        "logs/{scenario}/i{iteration}/y{year}/import_REMIND_costs.log",
    benchmark:
        "benchmarks/{scenario}/i{iteration}/y{year}/import_REMIND_costs"
    group:
        "iy"
    script:
        "scripts/import_REMIND_costs.py"


# Build load data time-series based on REMIND annual load per region and default PyPSA-EUR load profiles.
rule import_REMIND_load:
    input:
        load_timeseries=RESOURCES + "load.csv",
        region_mapping="config/regionmapping_21_EU11.csv",
        remind_data="resources/{scenario}/i{iteration}/REMIND2PyPSAEUR.gdx",
    output:
        load_timeseries="resources/{scenario}/i{iteration}/y{year}/load.csv",
    log:
        "logs/{scenario}/i{iteration}/y{year}/import_REMIND_load.log",
    benchmark:
        "benchmarks/{scenario}/i{iteration}/y{year}/import_REMIND_load"
    group:
        "iy"
    script:
        "scripts/import_REMIND_load.py"


# Build csv with min capacities for PyPSA-EUR based on installed capacities from REMIND.
# Used as RCL (Region-Carrier-Limits) contraints in PyPSA-EUR.
rule import_REMIND_RCL_p_nom_limits:
    input:
        remind_data="resources/{scenario}/i{iteration}/REMIND2PyPSAEUR.gdx",
        region_mapping="config/regionmapping_21_EU11.csv",
        technology_mapping="config/technology_mapping.csv",
    output:
        RCL_p_nom_limits="resources/{scenario}/i{iteration}/y{year}/RCL_p_nom_limits.csv",
    log:
        "logs/{scenario}/i{iteration}/y{year}/import_REMIND_RCL_p_nom_limits.log",
    benchmark:
        "benchmarks/{scenario}/i{iteration}/y{year}/import_REMIND_RCL_p_nom_limits"
    group:
        "iy"
    script:
        "scripts/import_REMIND_RCL_p_nom_limits.py"


# CO2 price is integrated into model either via config.yaml or via a wildcard
# we do not want to modify the config.yaml, so we use an approach to pass the CO2 price via a wildcard.
# For this we make use of the conditional execution from snakemake via checkpoints (see https://snakemake.readthedocs.io/en/stable/snakefiles/rules.html#data-dependent-conditional-execution), were first the checkpoint is executed
# and then we construct based on the checkpoint results the actual rule call and its input which will then solve the networks
# with their respective CO2 prices:
# 1. checkpoint determine_co2_price_scenarios: extract CO2 prices from REMIND + combine with scenario from config.yaml
# 2. rule solve_all_scenarios: Use the output of the checkpoint to solve all scenarios with specific CO2 prices
checkpoint determine_co2_price_scenarios:
    input:
        region_mapping="config/regionmapping_21_EU11.csv",
        remind_data="resources/{scenario}/i{iteration}/REMIND2PyPSAEUR.gdx",
    output:
        co2_price_scenarios="resources/{scenario}/i{iteration}/co2_price_scenarios.csv",
    log:
        "logs/{scenario}/i{iteration}/determine_co2_price_scenarios.log",
    benchmark:
        "benchmarks/{scenario}/i{iteration}/determine_co2_price_scenarios"
    script:
        "scripts/determine_CO2_price_scenarios.py"


# snakemake -n -call -s Snakefile_remind solve_all_scenarios --config remind_scenario=no_scenario iteration=
rule solve_all_scenarios:
    input:
        networks=lambda wildcards: expand(
            "results/{scenario}/i{iteration}/y{year}/networks/elec_s{simpl}_{clusters}_ec_l{ll}_{opts}.nc",
            zip,
            **pd.read_csv(
                checkpoints.determine_co2_price_scenarios.get(
                    scenario=wildcards["scenario"],
                    iteration=wildcards["iteration"],
                ).output["co2_price_scenarios"],
                na_filter=False,
            ).to_dict(orient="list")
        ),


# TODO align --config remind_scenario= iteration= with
# calls to rules with wildcards like below
rule extract_coupling_parameters:
    input:
        networks=rules.solve_all_scenarios.input["networks"],
        region_mapping="config/regionmapping_21_EU11.csv",
        technology_mapping="config/technology_mapping.csv",
        remind_weights="resources/{scenario}/i{iteration}/REMIND2PyPSAEUR.gdx",
    output:
        capacity_factors="results/{scenario}/i{iteration}/coupling-parameter/capacity_factors.csv",
        generation_shares="results/{scenario}/i{iteration}/coupling-parameters/generation_shares.csv",
        installed_capacities="results/{scenario}/i{iteration}/coupling-parameters/installed_capacities.csv",
        market_values="results/{scenario}/i{iteration}/coupling-parameters/market_values.csv",
        electricity_prices="results/{scenario}/i{iteration}/coupling-parameters/electricity_prices.csv",
        gdx="results/{scenario}/i{iteration}/coupling-parameters/PyPSAEUR2REMIND.gdx",
    log:
        "logs/{scenario}/i{iteration}/extract_coupling_parameters.log",
    benchmark:
        "benchmarks/{scenario}/i{iteration}/extract_coupling_parameters/"
    script:
        "scripts/extract_coupling_parameters.py"
