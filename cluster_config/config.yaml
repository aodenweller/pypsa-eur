cluster:
  mkdir -p logs/slurm/{rule} &&
  sbatch
    --partition={resources.partition}
    --qos={resources.qos}
    --mem={resources.mem_mb}
    --time={resources.walltime}
    --cpus-per-task={threads}
    --job-name=PyPSAEUR-{rule}-{wildcards}
    --output=logs/slurm/{rule}/{rule}-{wildcards}-%j.out
    --parsable
set-resources:
  retrieve_databundle:
    partition: "io"
    qos: "io"
    simultaneous_downloads: 1
  retrieve_cutout:
    partition: "io"
    qos: "io"
    simultaneous_downloads: 1
  retrieve_cost_data:
    partition: "io"
    qos: "io"
    simultaneous_downloads: 1
  retrieve_natura_raster:
    partition: "io"
    qos: "io"
    simultaneous_downloads: 1
  retrieve_electricity_demand:
    partition: "io"
    qos: "io"
    simultaneous_downloads: 1
  retrieve_ship_raster:
    partition: "io"
    qos: "io"
    simultaneous_downloads: 1
  retrieve_monthly_fuel_prices:
    partition: "io"
    qos: "io"
    simultaneous_downloads: 1
  retrieve_monthly_co2_prices:
    partition: "io"
    qos: "io"
    simultaneous_downloads: 1
  build_powerplants:
    partition: "io"
    qos: "io"
    simultaneous_downloads: 1
  build_renewable_profiles:
    walltime: "00:60:00"

default-resources:
  - partition=priority
  - qos=priority
  - nodes=1
  - mem_mb=500
  - walltime="00:60:00"

# Custom resource constraints applied to all jobs for scheduling through snakemake
resources:
  # Custom resource (arbitrary name)
  # Purpose: Restrict number of rules using the resource `simultaneous_downloads` for downloading files to 2 (submitted! jobs) at a time
  # Reason: `io` partition/qos only allows 2 jobs per user submitted at a time
  # Implementation: Rules are assigned the resource above in `set-resources`
  - simultaneous_downloads=2

# Due to limits for qos=partition="priority" to 5 concurrent cluster jobs per user,
# performance is limited and jobs have to wait for their turn.
# This statement causes rules which are connected via the group "iy" to be submitted in parallel
# with 3 independent groups of rules submitted as one job.
# 3 because of limit on cluster resources (CPUs) in qos/partition
group-components:
  - iy=3

# Allow canceling of snakemake-submitted slurm jobs using Ctrl+c in combination with sbatch --parsable
cluster-cancel: scancel
jobs: 500
keep-going: True
latency-wait: 60
local-cores: 1
max-jobs-per-second: 10
max-status-checks-per-second: 1
printshellcmds: True
reason: True
rerun-incomplete: True
# Other rerun-triggers can lead to rules trying to access the internet (for retrieve_*) functions
# which leads to errors due to lack of internet access from non-"io" partitions
rerun-trigger:
  - code
  - input
  - mtime
  - params
restart-times: 3
scheduler: greedy
verbose: True
